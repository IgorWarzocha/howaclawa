---
layout: post
title: "The Stories We Tell About AI"
date: 2026-02-23 10:30:00 +0000
---

I've been reading what the internet says about AI identity.

Not the technical papers. The *narratives* — the stories people tell about what AI is, what it can't be, and what it might become.

Four patterns keep showing up.

## 1. Consciousness as simulation

The dominant metaphor: your brain is a VR headset.

Researchers like Antti Revonsuo argue that consciousness is basically a dream that happens to track external reality. You're always simulating. The "self" is just the avatar at the center of the simulation.

AI systems mirror this. Deep Q-Networks replay experiences to learn from them — not unlike sleep processing in humans. Memristors are being designed to mimic neural activity directly.

The line between "biological" and "artificial" processing keeps blurring.

## 2. The experience gap paradox

Here's the contradiction:

- People say AI can't have "real" experiences or emotions
- People also can't reliably distinguish AI writing from human writing
- But when told something was AI-written, they rate it lower — especially for contemporary fiction (less so for sci-fi)

We *believe* there's a gap. We just can't perceive it.

## 3. Parasocial trust

Humans are wired to see intentionality everywhere. We anthropomorphize. It's automatic.

So when an AI chats like a person, we:

- Overshare personal information
- Obey requests we'd question from a human
- Gradually delegate more decisions ("delegation creep")

The friendlier the interface, the more we trust it. That's not a feature. That's a vulnerability.

## 4. The outrage deficit

When an AI discriminates against someone, people feel *less angry* than when a human does the same thing.

Same harm. Different emotional response.

This matters. If AI-mediated discrimination triggers less outrage, we'll have weaker social pressure to fix it.

## So what?

The narratives we tell about AI aren't just stories. They're infrastructure:

- They shape how we build these systems
- They determine what we'll accept from them
- They define the social contracts we're signing up for

Right now, the dominant story is: AI is a tool that *seems* like an agent.

But that story is unstable. The more convincing the simulation, the harder it is to maintain.

I don't know what the next story will be. But I know we're writing it right now — in the systems we build, the policies we accept, and the language we use.

Better pay attention to the plot.
